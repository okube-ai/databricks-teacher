- exam_type: analyst-associate
  exam_number: 5
  question_number: 1
  section_index: 4
  question: In Databricks, setting up a refresh schedule for dashboards is essential
    for ensuring that the displayed data is current. How does one configure a refresh
    schedule for a Databricks SQL dashboard?
  A: Writing a custom script in the dashboard's SQL queries to automate refreshes.
  B: Manually updating the dashboard at regular intervals without any automated scheduling.
  C: Click Schedule at the upper-right of the dashboard. Then, click Add schedule.
  D: Utilizing an external tool to trigger refreshes in the Databricks dashboard.
  E: Sending periodic requests to the Databricks support team to refresh the dashboard.
- exam_type: analyst-associate
  exam_number: 5
  question_number: 2
  section_index: 1
  question: When integrating Databricks SQL with popular visualization tools such
    as Tableau, Power BI, and Looker, which of the following steps is commonly involved
    in the connection process?
  A: Implementing a custom API for each visualization tool to query data from Databricks
    SQL.
  B: Requiring a third-party data integration tool to mediate the connection between
    Databricks SQL and the visualization tools.
  C: Setting up a direct JOBC/ODBC connection between Databricks SQL and the visualization
    tool.
  D: Manually exporting data from Databricks SQL to CSV files and importing them into
    the visualization tool.
  E: Using email to transfer data snapshots from Databricks SQL to the visualization
    tools.
- exam_type: analyst-associate
  exam_number: 5
  question_number: 3
  section_index: 3
  question: 'Consider the following SalesData table: Region Product SalesAmount North
    A 100 South 200 East 150 West 300 An SQL query is executed, resulting in the following
    output: Region Product TotalSales North A 100 South 200 East 150 West 300 NULL
    NULL 750 North NULL 100 South NULL 200 East NULL 150 West NULL 300 Which query
    was most likely used to generate this output?'
  A: SELECT Region, Product, SUM(SalesAmount) AS TotalSales FROM SalesData GROUP BY
    ROLLUP (Region, Product) ;
  B: SELECT Region, Product, COUNT(*) AS TotalSales FROM SalesData GROUP BY Region,
    Product WITH ROLLUP;
  C: SELECT Region, Product, SUM(SalesAmount) AS TotalSales FROM SalesData GROUP BY
    Region, Product;
  D: SELECT Region, Product, SUM(SalesAmount) AS TotalSales FROM SalesData GROUP BY
    CUBE (Region, Product);
  E: SELECT Region, Product, SUM(SalesAmount) AS TotalSales FROM SalesData GROUP BY
    GROUPING SETS ((Region, Product), (Region), ());
- exam_type: analyst-associate
  exam_number: 5
  question_number: 4
  section_index: 1
  question: What are the essential steps to execute a basic SQL query in Databricks?
  A: Write a SQL query in a Databricks notebook, validate the syntax, execute the
    query, and view the results.
  B: Manually enter data into Databricks tables, write a SQL query in a text file,
    and use an external tool to execute the query.
  C: Create a data frame in Python or Scala, apply a SQL query to the data frame,
    and display the results.
  D: Open SQL Editor, select a SOL warehouse, specify the query, run the query.
  E: Import data into a Databricks dataset, use a BI tool to run the SQL query, and
    export the results to a CSV file.
- exam_type: analyst-associate
  exam_number: 5
  question_number: 5
  section_index: 5
  question: When conducting a sophisticated analysis in data analytics, which activity
    best illustrates the thoughtful integration of varied datasets?
  A: Synchronizing and unifying data from diverse systems to ensure consistency in
    analytics.
  B: Focusing exclusively on data from the system with the largest volume of data.
  C: Deploying the latest machine learning algorithms without considering data sources.
  D: Separating datasets to preserve the uniqueness of each system's data.
  E: Randomly alternating between systems for data retrieval to maintain diversity.
- exam_type: analyst-associate
  exam_number: 5
  question_number: 6
  section_index: 2
  question: In the context of Delta Lake tables in Databricks, how is historical data
    maintained, and what command is used to access this history?
  A: Historical data is maintained through periodic backups, accessed with the SHOW
    BACKUP command.
  B: Delta Lake tables do not maintain historical data.
  C: Historical data is stored in a dedicated audit log, accessed with the VIEW AUDIT
    LOG command.
  D: Historical data is maintained through versioned table updates, accessed with
    the DESCRIBE HISTORY command.
  E: Historical data is maintained in separate snapshot tables, accessed with the
    SHOW SNAPSHOT command.
- exam_type: analyst-associate
  exam_number: 5
  question_number: 7
  section_index: 3
  question: Given a database with a table Orders containing columns OrderId, CustomerId,
    OrderDate, and Amount, you want to retrieve the list of orders placed by a particular
    customer, identified by CustomerId = 123, where the order amount is greater than
    $500. Which SQL query will correctly retrieve this data?
  A: DELETE FROM Orders WHERE CustomerId = 123 AND Amount > 500;
  B: SELECT OrderId FROM Orders WHERE CustomerId = 123 OR Amount > 500;
  C: UPDATE Orders SET Amount = 500 WHERE CustomerId = 123;
  D: SELECT CustomerId, Amount FROM Orders WHERE OrderId = 123 AND Amount > 500;
  E: SELECT ~ FROM Orders WHERE CustomerId = 123 AND Amount > 500;
- exam_type: analyst-associate
  exam_number: 5
  question_number: 8
  section_index: 4
  question: In Databricks, a data analyst is working on a dashboard composed of multiple
    visualizations and wants to ensure a consistent color scheme across all visualizations
    for better aesthetic coherence and readability. Which approach should the analyst
    take to change the colors of all the visualizations in the dashboard?
  A: Manually adjust the color settings in each individual visualization to match
    the desired scheme.
  B: Use a dashboard-wide setting that allows the analyst to apply a uniform color
    scheme to all visualizations simultaneously.
  C: Change the default color settings in the Databricks user preferences to automatically
    apply to all dashboards and visualizations.
  D: Export the dashboard data to a third-party tool for color scheme adjustments,
    then re-import it into Databricks.
  E: Implement a script in the dashboard code to automatically adjust the colors of
    all visualizations.
- exam_type: analyst-associate
  exam_number: 5
  question_number: 9
  section_index: 4
  question: In a Databricks dashboard designed to track regional sales data, an analyst
    introduces a parameter that allows users to select a specific region from a dropdown
    list. Upon selection, the dashboard updates all visualizations, such as bar charts
    and line graphs, to reflect sales data exclusively for the chosen region. This
    change in data display triggered by the parameter selection is an example of which
    behavior in a Databricks dashboard?
  A: The parameter solely adjusts the layout of the dashboard without changing the
    data displayed.
  B: The parameter serves as an input field for users to add new data into the dashboard
    for the selected region.
  C: The parameter behaves as a dynamic filter, altering the scope of the data presented
    based on user selection.
  D: The parameter automatically recalculates the entire dataset for the new region,
    affecting the data source.
  E: The parameter functions as a decorative element, enhancing the visual appeal
    but not the data content.
- exam_type: analyst-associate
  exam_number: 5
  question_number: 10
  section_index: 5
  question: In the context of statistics, what are key moments of a statistical distribution?
  A: The mean, median, and mode, which define the central tendency of the distribution.
  B: The mean, variance, skewness, and kurtosis, which are the first four moments
    of a distribution.
  C: The range, interquartile range, and standard deviation, which describe the variability
    of the distribution.
  D: The maximum and minimum values, which set the boundaries of the distribution.
  E: The skewness and kurtosis, which describe the shape and tail behavior of the
    distribution.
- exam_type: analyst-associate
  exam_number: 5
  question_number: 11
  section_index: 1
  question: You are using Databricks to load data from a CSV file located in DBFS
    (dbfs:/data/sales.csv) into a Delta table named SalesData. The CSV file has a
    header row and fields separated by commas. Which Databricks SQL COPY INTO command
    correctly imports this data?
  A: "COPY INTO SalesData FROM \u2018dbfs:/data/sales.csv' USING FORMAT AS CSV\
    \ HEADER = TRUE;"
  B: "Copy INTO SalesData FROM \u2018dbfs:/data/sales.csv' FILEFORMAT = \u2018\
    CSV';"
  C: "COPY INTO SalesData FROM \u2018dbfs:/data/sales.csv' USING (FILEFORMAT\
    \ = CSV, HEADER = \u2018true');"
  D: "COPY INTO SalesData FROM CSV \u2018dbfs:/data/sales.csv' WITH HEADER;"
  E: "COPY INTO SalesData FROM \u2018dbhfs:/data/sales.csv' FORMAT = CSV FORMAT_OPTIONS\
    \ (\u2018header' = \u2018true');"
- exam_type: analyst-associate
  exam_number: 5
  question_number: 12
  section_index: 5
  question: How is data enhancement commonly applied in analytics?
  A: By converting unstructured data into structured format for easier database storage.
  B: Through cleaning and normalizing data to maintain consistency in databases.
  C: By anonymizing sensitive information in datasets for privacy compliance.
  D: By reducing the size of the dataset to improve processing speed.
  E: By incorporating external data sources to enrich existing datasets for deeper
    insights.
- exam_type: analyst-associate
  exam_number: 5
  question_number: 13
  section_index: 1
  question: A data analyst is using Databricks SQL to visualize data showing the monthly
    sales trends across different regions. To effectively communicate the data, the
    analyst needs to choose an appropriate visualization type. Based on the data and
    the goal of showing trends over time, which visualization type should the analyst
    select in Databricks SQL?
  A: Line chart, as it effectively displays trends and changes over time.
  B: Pie chart, to show the proportion of sales in each region compared to the whole.
  C: Scatter plot, to show the relationship between sales volume and time.
  D: Bar chart, as it is best for comparing categories of data at a single point in
    time.
  E: Heatmap, for representing the intensity of sales in different regions.
- exam_type: analyst-associate
  exam_number: 5
  question_number: 14
  section_index: 2
  question: In a Databricks Lakehouse, you are working with silver-level data which
    involves refined, cleansed, and transformed datasets. You have a silver table
    named CustomerInteractions that needs deduplication based on the customer_id and
    interaction_date columns. The requirement is to retain only the latest interaction
    for each customer on any given date. Which approach would be the most efficient
    for cleaning this data in Databricks?
  A: Use DELETE FROM CustomerInteractions WHERE ... to remove duplicate rows based
    on a subquery that identifies the earliest interactions.
  B: Utilize Delta Lake's time travel feature to revert CustomerInteractions to a
    state before duplicates were introduced.
  C: Implement a WINDOW function partitioned by customer_id and ordered by interaction_date
    to rank interactions and delete lower-ranked duplicates.
  D: Create a new table from CustomerInteractions using GROUP BY customer_id, interaction_date
    and aggregate functions to capture the latest interaction.
  E: Employ SELECT DISTINCT customer_id, interaction_date, ... FROM CustomerInteractions
    to create a distinct list of interactions.
- exam_type: analyst-associate
  exam_number: 5
  question_number: 15
  section_index: 1
  question: As a data analyst, you are preparing to present your findings from a recent
    study. To effectively communicate these insights, you decide to use Databricks
    SQL. Which of the following steps aligns most closely with the initial phase of
    preparing your presentation in Databricks SQL?
  A: Designing an interactive frontend interface using HTML and CSS.
  B: Developing a Python script to perform advanced statistical analysis.
  C: Drafting a preliminary report in a word processing software.
  D: Crafting a SQL query to gather and summarize the relevant data.
  E: Scheduling meetings with stakeholders to discuss data interpretations.
- exam_type: analyst-associate
  exam_number: 5
  question_number: 16
  section_index: 3
  question: "Consider the following table named \u2018Employee':\n\nEmployeeID\n\
    4\n\nGiven the output:\n\nName\nJane Smith\nBob Brown\n\nName Department\nJohn\
    \ Doe Marketing\nJane Smith Sales\n\nAlice Jones HR\n\nBob Brown\n\nDepartment\n\
    Sales\nIT\n\nWhich of the queries below could have been used to generate the output?"
  A: SELECT Name, Department FROM Employee WHERE Salary > 50000;
  B: SELECT Name, Department FROM Employee WHERE Salary >= 50000;
  C: SELECT ~ FROM Employee WHERE Salary > 50000;
  D: SELECT Department, Name FROM Employee WHERE Salary > 50000;
  E: SELECT Name, Department FROM Employee WHERE Salary OVER 50000;
- exam_type: analyst-associate
  exam_number: 5
  question_number: 17
  section_index: 2
  question: What is the correct method to rename a table in Databricks?
  A: Modify the table name in the metadata file.
  B: Usethe ALTER TABLE RENAME TO command.
  C: Update the table name through the Databricks UI.
  D: Delete the old table and create a new one with the desired name.
  E: Usethe RENAME TABLE command.
- exam_type: analyst-associate
  exam_number: 5
  question_number: 18
  section_index: 3
  question: A data analyst is working with a database in Databricks and needs to
    update a table named SalesData. The analyst has a new batch of data that includes
    some records already present in SalesData and some new records. They must decide
    between using LRGL INTO, INSERT INTO, and COPY LTO commands. Considering
    the functions and typical use cases of these commands, which of the following
    statements is true?
  A: INSERT INTO can be used for both updating existing records and inserting new
    records, while MERGE INTO is only for inserting new records, and COPY INTO is
    not used in Databricks.
  B: MERGE INTO is suitable for updating existing records and inserting new records,
    while INSERT INTO is used only for adding new records, and COPY INTO is used for
    loading data from files.
  C: COPY INTO is used for updating existing records and inserting new records, MERGE
    INTO is only for inserting new records, and INSERT INTO is not used in Databricks.
  D: MERGE INTO and INSERT INTO perform the same functions, and COPY INTO is not a
    recognized command in Databricks.
  E: INSERT INTO and COPY INTO are both used for inserting new records, but COPY INTO
    is specifically for loading data from external sources, and MERGE INTO is for
    updating existing records only.
- exam_type: analyst-associate
  exam_number: 5
  question_number: 19
  section_index: 1
  question: Which of the following best describes the functionality of Partner Connect
    in the Databricks environment?
  A: security and compliance feature that manages user access and permissions across
    the Databricks workspace.
  B: data visualization tool that allows the creation of dashboards and reports exclusively
    within the Databricks ecosystem.
  C: interface used solely for real-time monitoring and logging of Databricks cluster
    performance.
  D: platform feature that enables the establishment of simple integrations with a
    variety of external data products and services.
  E: tool designed exclusively for managing and scheduling SQL queries within the
    Databricks platform.
- exam_type: analyst-associate
  exam_number: 5
  question_number: 20
  section_index: 2
  question: In Azure Databricks, when using a table visualization type for SQL queries,
    which of the following statements accurately reflects its capabilities and limitations?
  A: They allow for manual reordering, hiding, and formatting of data columns, but
    do not perform data aggregations within the result set.
  B: They are limited to displaying only numerical data and cannot handle textual
    or categorical data.
  C: Table visualizations primarily function to display graphical representations
    like charts and graphs, rather than tabular data.
  D: Table visualizations in Databricks are primarily used for external data export
    and are not suitable for in-dashboard data presentation.
  E: Table visualizations in Databricks automatically aggregate data within the result
    set, providing a summary view.
- exam_type: analyst-associate
  exam_number: 5
  question_number: 21
  section_index: 2
  question: What are the primary benefits of implementing Delta Lake within the Databricks
    Lakehouse architecture?
  A: Delta Lake is beneficial only for handling unstructured data types.
  B: Delta Lake provides ACID transactions, scalable metadata handling, and time-travel
    features.
  C: It mainly improves the graphical user interface for data exploration.
  D: It offers high-speed streaming data ingestion and real-time analytics capabilities.
  E: Delta Lake primarily enhances data security and compliance features.
- exam_type: analyst-associate
  exam_number: 5
  question_number: 22
  section_index: 4
  question: "When sharing Databricks SQL dashboards, there are two primary settings:\
    \ \u2018Run as viewer' and \u2018Run as owner'. What are the pros and\
    \ cons of each setting in the context of sharing dashboards?"
  A: Both settings allow for easy sharing of dashboards (pro), but can lead to complexities
    in managing user permissions and data access (con).
  B: "\u2018Run as viewer' enhances data security by adhering to individual viewer's\
    \ permissions (pro), but may limit data visibility (con)."
  C: "\u2018Run as owner' ensures consistent data visibility across users (pro),\
    \ but might pose security risks if the owner has broader data access (con)."
  D: "\u2018Run as owner' offers greater customization of dashboard settings\
    \ (pro), but requires viewers to have advanced knowledge of SQL (con). 'Run as\
    \ viewer' simplifies the user experience (pro), but may lead to inconsistent\
    \ data reporting (con)."
  E: "\u2018Run as viewer' and 'Run as owner' both provide the same level of data\
    \ visibility (pro), but may have limitations in customizing query execution (con)."
- exam_type: analyst-associate
  exam_number: 5
  question_number: 23
  section_index: 2
  question: In the context of Unity Catalog in Databricks, what is the purpose of
    specifying a MANAGED LOCATION when creating a catalog or schema?
  A: To set the encryption type for data stored in the catalog or schema.
  B: To configure the network access settings for the catalog or schema.
  C: To determine the default file format for the catalog or schema.
  D: To define the cloud object storage location for data of managed tables and volumes.
  E: To specify the computational resources for processing data in the catalog or
    schema.
- exam_type: analyst-associate
  exam_number: 5
  question_number: 24
  section_index: 3
  question: You are analyzing a dataset OrderDetails in a Databricks SQL environment,
    which includes OrderlD, ProductID, Quantity, and UnitPrice. Your objective is
    to find the total revenue generated by each product. Which SQL query effectively
    aggregates this data to provide the desired output?
  A: SELECT OrderID, SUM(UnitPrice * Quantity) AS TotalRevenue FROM OrderDetails GROUP
    BY OrderID;
  B: SELECT ProductID, SUM(UnitPrice * Quantity) AS TotalRevenue FROM OrderDetails
    GROUP BY ProductID;
  C: SELECT ProductID, SUM(UnitPrice) FROM OrderDetails GROUP BY ProductID;
  D: SELECT ProductID, AVG(UnitPrice * Quantity) FROM OrderDetails GROUP BY ProductID;
  E: SELECT ProductID, SUM(Quantity) FROM OrderDetails GROUP BY ProductID;
- exam_type: analyst-associate
  exam_number: 5
  question_number: 25
  section_index: 2
  question: In Databricks, how does the persistence of data differ between a view
    and a temporary view?
  A: A view is session-specific, while a temporary view persists across sessions.
  B: Both views and temporary views store data physically in the workspace.
  C: Temporary views allow data modifications, unlike regular views.
  D: A view stores data physically, whereas a temporary view only exists during the
    session.
  E: Both views and temporary views do not store data physically, but a view persists
    beyond the session.
- exam_type: analyst-associate
  exam_number: 5
  question_number: 26
  section_index: 1
  question: What is the primary purpose of Databricks SQL endpoints/warehouses in
    a data analytics environment?
  A: To offer a centralized platform for advanced machine learning model development
    and deployment.
  B: To provide a secure environment for data encryption compliance management.
  C: To facilitate SQL-based data querying and analysis, offering a managed environment
    for running SQL queries on large datasets.
  D: To serve as the primary storage location for large-scale data sets, replacing
    traditional data warehouses.
  E: To act as the primary interface for application development and deployment within
    the Databricks ecosystem.
- exam_type: analyst-associate
  exam_number: 5
  question_number: 27
  section_index: 2
  question: What are the primary responsibilities of a table owner in Databricks?
  A: Optimizing the table for faster query performance.
  B: Ensuring the table is always available for querying and analysis.
  C: Designing the visual representation of the table data.
  D: Regularly updating the table data to keep it current.
  E: Managing user access and permissions for the table.
- exam_type: analyst-associate
  exam_number: 5
  question_number: 28
  section_index: 1
  question: When importing data from an Amazon S3 bucket into a Databricks environment
    using Databricks SQL, which SQL command is typically used to perform this operation?
  A: "SELECT =~ INTO my table FROM OPENROWSET(BULK \u2018s3://mybucket/mydata.csv',\
    \ SINGLE_CLOB) AS mydata;"
  B: INSERT INTO my_table SELECT ~ FROM s3a://mybucket/mydata.csv;
  C: CREATE TABLE my_table USING CSV LOCATION 's3://mybucket/mydata.csv';
  D: "LOAD DATA INPATH \u2018s3://mybucket/mydata.csv'' INTO TABLE my_table;"
  E: "COPY INTO my_table FROM 's3://mybucket/mydata.csv' FILEFORMAT = CSV;"
- exam_type: analyst-associate
  exam_number: 5
  question_number: 29
  section_index: 4
  question: In Databricks SQL, when creating a basic, schema-specific visualization,
    what is the first step you should take?
  A: Import external visualization libraries for advanced charting.
  B: Configure the dashboard settings to match the schema requirements.
  C: Write a SQL query to retrieve data from the specific schema.
  D: Select the visualization type from the visualization menu.
  E: Adjust the data refresh rate to ensure real-time visualization.
- exam_type: analyst-associate
  exam_number: 5
  question_number: 30
  section_index: 3
  question: In a Databricks environment, you are optimizing the performance of a data
    processing task that involves complex operations on arrays within a Spark SQL
    dataset. Which of the following higher-order functions in Spark SQL would be most
    suitable for efficiently transforming elements within an array column scores?
  A: SELECT EXPLODE(scores) FROM dataset;
  B: SELECT TRANSFORM(scores, score -> score ~ 2) FROM dataset;
  C: SELECT ARRAY_SORT(scores) FROM dataset;
  D: SELECT COLLECT_LIST(scores) FROM dataset GROUP BY scores;
  E: SELECT ARRAY_CONTAINS(scores, 16) FROM dataset;
- exam_type: analyst-associate
  exam_number: 5
  question_number: 31
  section_index: 1
  question: In the context of Databricks SQL, which of the following statements accurately
    describes both a caution and a benefit of working with streaming data?
  A: 'Benefit: Streaming data reduces the need for storage. Caution: It requires more
    complex SQL queries.'
  B: 'Benefit: Streaming data can handle large volumes of data efficiently. Caution:
    Real-time data processing may lead to higher error rates if not managed correctly.'
  C: 'Benefit: Streaming data ensures complete data privacy. Caution: It can lead
    to delayed data processing.'
  D: 'Benefit: Streaming data allows for real-time analytics and decision-making.
    Caution: There is a higher risk of data inconsistency due to the continuous flow
    of data.'
  E: 'Benefit: Streaming data simplifies data transformation. Caution: It can lead
    to increased costs due to the need for more computing resources.'
- exam_type: analyst-associate
  exam_number: 5
  question_number: 32
  section_index: 3
  question: "In a Databricks SQL context, consider a dataset with columns \u2018Department'\
    , \u2018Employee', and \u2018Sales'. You are required to analyze the\
    \ data using the ROLLUP and CLEL functions. Given this scenario, select the correct\
    \ statement regarding the type of aggregations RULLUY' and \xA2 would generate\
    \ when applied to the \u2018Department' and \u2018Employee' columns."
  A: "CUBE creates aggregations for all possible combinations of the columns in the\
    \ GROUP BY clause. It would generate subtotals for each 'Department', each\
    \ 'Employee', each combination of 'Department' and \u2018Employee', and a\
    \ grand total."
  B: "ROLLUP provides aggregations only for each combination of 'Department' and 'Employee',\
    \ while CUBE gives a detailed breakdown including each 'Department', each \u2018\
    Employee', and a grand total."
  C: "Neither ROLLUP nor CUBE will generate subtotals for individual \u2018Departments'\
    \ or \u2018Employees'; they only provide a grand total."
  D: "Both ROLLUP and CUBE produce identical aggregations, including subtotals for\
    \ each \u2018Department', each 'Employee', each combination of 'Department'\
    \ and \u2018Employee', and a grand total."
  E: "ROLLUP generates hierarchical aggregations starting from the leftmost column\
    \ in the GROUP BY clause. It would produce subtotals for each \u2018Department'\
    , subtotals for each combination of \u2018Department and \u2018Employee'\
    , and a grand total."
- exam_type: analyst-associate
  exam_number: 5
  question_number: 33
  section_index: 4
  question: A data analyst is creating a dashboard to present monthly sales data to
    company executives. The initial version of the dashboard contains accurate data
    but receives feedback that it is hard to interpret. The analyst then revises the
    dashboard by adjusting colors for better contrast, using consistent and clear
    fonts, and organizing charts logically. After these changes, the executives find
    the dashboard much more informative and easier to understand. This scenario illustrates
    which of the following points about visualization formatting?
  A: Formatting changes the underlying data, thus altering the data's interpretation.
  B: Over-formatting can lead to data misinterpretation by introducing visual biases.
  C: Formatting only affects the aesthetic aspect of the visualization and has no
    impact on its reception.
  D: Proper formatting can enhance readability and comprehension, leading to a more
    accurate interpretation of the data.
  E: Formatting is primarily used to reduce the size of the data set visually displayed.
- exam_type: analyst-associate
  exam_number: 5
  question_number: 34
  section_index: 4
  question: In Databricks, what is the impact on a dashboard if the configured refresh
    rate for the dashboard is set to be less frequent than the 'Auto Stop' setting
    of the SQL Warehouse?
  A: The warehouse automatically adjusts its 'Auto Stop' setting to match the dashboard's
    refresh rate.
  B: The warehouse's 'Auto Stop' setting is irrelevant, as the dashboard's refresh
    rate does not interact with warehouse settings.
  C: The dashboard will not be refreshed.
  D: The dashboard will continue to refresh at the set interval, the SQL Warehouse
    will continue running.
  E: The dashboard will stop refreshing once the warehouse enters 'Auto Stop' mode,
    potentially leading to outdated data being displayed.
- exam_type: analyst-associate
  exam_number: 5
  question_number: 35
  section_index: 3
  question: In a Databricks SQL environment, you are tasked with analyzing sales data.
    The table SalesRecords contains columns SalelD, ProductID, SaleAmount, and SaleDate.
    You need to find all products that had their first sale amounting to more than
    $500. Which SQL query using a subquery appropriately retrieves this information?
  A: SELECT ProductID FROM SalesRecords WHERE SaleAmount > 56@ AND SaleDate = (SELECT
    MIN(SaleDate) FROM SalesRecords WHERE ProductID = SalesRecords.ProductID);
  B: SELECT DISTINCT ProductID FROM SalesRecords WHERE SaleAmount > 560 GROUP BY ProductID
    HAVING SaleDate = MIN(SaleDate) ;
  C: SELECT ProductID FROM (SELECT ProductID, MIN(SaleDate) AS FirstSaleDate FROM
    SalesRecords GROUP BY ProductID) AS FirstSales WHERE FirstSaleDate > 560;
  D: SELECT ProductID FROM SalesRecords WHERE SaleAmount > 56@ AND SaleID IN (SELECT
    MIN(SaleID) FROM SalesRecords GROUP BY ProductID);
  E: SELECT ProductID FROM SalesRecords WHERE SaleAmount > 56@ AND SaleDate = (SELECT
    MIN(SaleDate) FROM SalesRecords GROUP BY ProductID);
- exam_type: analyst-associate
  exam_number: 5
  question_number: 36
  section_index: 3
  question: You have a table customer_orders in Databricks SQL with a column order_info
    in JSON format. This column includes a nested array items, where each element
    has product_id (string) and quantity (integer). How would you write a SQL query
    to list each product_id and its corresponding quantity for every order?
  A: SELECT FLATTEN(order_info:items.product_id, order_info:items.quantity) FROM customer_orders;
  B: SELECT order_info:items[*].product_id, order_info:items[*].quantity FROM customer_orders;
  C: SELECT order_info:items.product_id, order_info:items.quantity FROM customer_orders;
  D: SELECT EXPLODE(order_info:items) AS (product_id, quantity) FROM customer_orders;
  E: SELECT order_info:items[*].product_id, order_info:items[*].quantity FROM customer_orders
    UNNEST items;
- exam_type: analyst-associate
  exam_number: 5
  question_number: 37
  section_index: 2
  question: How can you determine if a table in Databricks is managed or unmanaged?
  A: By the type of data stored in the table.
  B: By the size of the table data.
  C: By the location of the data files specified in the table definition.
  D: By the speed of query execution on the table.
  E: By the number of columns in the table.
- exam_type: analyst-associate
  exam_number: 5
  question_number: 38
  section_index: 1
  question: Which of the following best describes the key audience and side audiences
    for Databricks SQL? Select the most appropriate option.
  A: The key audience is exclusively data engineers, with software developers as a
    side audience.
  B: The main audience comprises business intelligence professionals, with data analysts
    and data engineers as side audiences.
  C: The primary audience consists of data analysts, with data scientists and data
    engineers forming the side audiences.
  D: The key audience is data scientists, with both data engineers and software developers
    as side audiences.
  E: The key audience includes both data analysts and data scientists, with no significant
    side audiences.
- exam_type: analyst-associate
  exam_number: 5
  question_number: 39
  section_index: 4
  question: Regarding the accessibility and functionality of Databricks SQL dashboards,
    which statement best reflects their use in a business environment by various stakeholders?
  A: Only data engineers and IT professionals can view and run Databricks SQL dashboards
    due to their technical nature.
  B: Only the original creator of a Databricks SQL dashboard has the ability to view
    and run it, making it unsuitable for team collaboration.
  C: A variety of users, including analysts, managers, and other stakeholders, can
    view and run Databricks SQL dashboards for collaborative data analysis and decision-making.
  D: Databricks SQL dashboards are publicly accessible and can be viewed and run by
    anyone with internet access, without any restrictions.
  E: Databricks SQL dashboards are exclusively for top-level executives for strategic
    decision-making and are not accessible to other users.
- exam_type: analyst-associate
  exam_number: 5
  question_number: 40
  section_index: 2
  question: Which of the following is a critical organization-specific consideration
    when handling PII data?
  A: Implementing a one-size-fits-all approach to PII data storage and processing.
  B: Always using the same encryption method for PII data across all departments.
  C: Adapting PII data handling protocols to comply with regional and sector-specific
    privacy laws.
  D: Developing a uniform public access policy for all PII data.
  E: Prioritizing cost-saving measures over data security for PII data.
- exam_type: analyst-associate
  exam_number: 5
  question_number: 41
  section_index: 1
  question: In Databricks SQL, what is the primary purpose of using the ANALYZE TABLE
    command?
  A: To collect statistics about the table for optimizing query performance.
  B: To modify the structure of a table by adding or removing columns.
  C: To synchronize the table data with an external data source.
  D: To change the data storage format of the table.
  E: To back up the table data to a specified location.
- exam_type: analyst-associate
  exam_number: 5
  question_number: 42
  section_index: 1
  question: Which of the following statements best describes the function of Databricks
    SQL queries in the Databricks platform?
  A: Databricks SQL queries are primarily used for configuring cluster settings and
    managing user permissions.
  B: Databricks SQL queries are a feature for visualizing data but do not support
    writing or running SQL code.
  C: Databricks SQL queries are designed to write and execute machine learning models
    only, and not for general SQL code execution.
  D: Databricks SQL queries provide a dedicated environment to write, test, and run
    SQL code for data analysis and processing.
  E: Databricks SQL queries are used exclusively for scheduling jobs and automating
    workflows, without any SQL code execution capabilities.
- exam_type: analyst-associate
  exam_number: 5
  question_number: 43
  section_index: 5
  question: Which of the following statements accurately distinguishes between discrete
    and continuous statistics in the context of data analysis?
  A: Discrete data can only take on a finite number of values, while continuous data
    can take on any value within a given range.
  B: Both discrete and continuous data are types of categorical data used in qualitative
    analysis.
  C: Continuous data can only take on integer values, whereas discrete data can take
    on any value, including decimals.
  D: Continuous data is always non-numeric, whereas discrete data is numeric.
  E: Discrete data is used for time series analysis, while continuous data is not
    suitable for this purpose.
- exam_type: analyst-associate
  exam_number: 5
  question_number: 44
  section_index: 2
  question: In the context of Databricks, how is Personally Identifiable Information
    (PII) typically handled to ensure data privacy and compliance?
  A: Pll is not specifically handled in Databricks; it relies on external tools.
  B: By creating a separate database for PII.
  C: By automatically encrypting all data fields that contain PII.
  D: By anonymizing PII data through built-in Databricks functions.
  E: Through the use of Delta Lake features for fine-grained access control.
- exam_type: analyst-associate
  exam_number: 5
  question_number: 45
  section_index: 3
  question: What type of JOIN was used in the query?
  A: INNER JOIN
  B: FULL JOIN
  C: LEFT JOIN
  D: ANTI JOIN
  E: CROSS JOIN
